{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Métodos Numéricos - Actividad Extra 12\"\n",
    "author: \"Alicia Pereira\"\n",
    "date: \"December 17, 2024\"\n",
    "format: \n",
    "  pdf:\n",
    "    documentclass: article\n",
    "    number-sections: true\n",
    "    include-in-header: header.tex\n",
    "toc: true\n",
    "toc-title: \"Contenido\"\n",
    "fontsize: 11pt\n",
    "linestretch: 1.5\n",
    "lang: es\n",
    "engine: pdflatex\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Informe: Web Scraping en Datos Abiertos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web scraping es una técnica de extracción de datos estructurados desde sitios web de manera automatizada. Permite recolectar información publicada en línea y almacenarla en formatos manejables como CSV o bases de datos para su posterior análisis. En este informe, se documenta el uso de dos herramientas populares para realizar web scraping: _BeautifulSoup_ y _Selenium_, aplicado al sitio datosabiertos.gob.ec, el portal de datos abiertos del gobierno de Ecuador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos\n",
    "\n",
    "- Comprender el funcionamiento de herramientas de web scraping en Python.\n",
    "\n",
    "- Realizar pruebas con las librerías BeautifulSoup y Selenium.\n",
    "\n",
    "- Extraer información de títulos y enlaces de datasets publicados en el sitio.\n",
    "\n",
    "- Guardar los datos obtenidos en un archivo CSV para su análisis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tecnologías y Herramientas\n",
    "\n",
    "**Librerías Utilizadas**\n",
    "\n",
    "1. _BeautifulSoup_: (parte de bs4): Para analizar el contenido HTML de la página web y extraer información estructurada.\n",
    "\n",
    "2. _Selenium_: Para interactuar con páginas web que cargan contenido dinámico mediante JavaScript.\n",
    "\n",
    "3. _Pandas_: Para manejar y guardar los datos en un formato CSV.\n",
    "\n",
    "4. _Requests_: Para realizar solicitudes HTTP y obtener el contenido HTML de la página.\n",
    "\n",
    "**Software**\n",
    "\n",
    "- Python 3.12.4\n",
    "\n",
    "- Navegador Google Chrome\n",
    "\n",
    "- ChromeDriver\n",
    "\n",
    "**Instalación de Librerías**\n",
    "\n",
    "Antes de ejecutar los scripts, es necesario instalar las librerías requeridas mediante el siguiente comando:\n",
    "\n",
    "    pip install requests beautifulsoup4 pandas selenium\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desarrollo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BeautifulSoup\n",
    "\n",
    "**Procedimiento:**\n",
    "\n",
    "1. Realizar una solicitud HTTP al sitio web [https://datosabiertos.gob.ec/](https://datosabiertos.gob.ec/) utilizando la librería requests.\n",
    "\n",
    "2. Analizar el contenido HTML con BeautifulSoup.\n",
    "\n",
    "3. Identificar los selectores CSS correspondientes a los títulos y enlaces de los datasets.\n",
    "\n",
    "4. Extraer los datos y guardarlos en un archivo CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos guardados en 'datasets_datosabiertos_bs.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL del sitio web\n",
    "url = \"https://datosabiertos.gob.ec/\"\n",
    "\n",
    "# Realizar la solicitud HTTP\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Asegura que no hubo errores en la solicitud\n",
    "\n",
    "# Analizar el contenido HTML\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Extraer los enlaces de los datasets\n",
    "datasets = []\n",
    "for item in soup.select(\"h3.dataset-title\"):  # Cambiar el selector según sea necesario\n",
    "    title = item.text.strip()\n",
    "    link = item.find_parent(\"a\").get(\"href\")  # Encuentra el enlace contenedor del título\n",
    "    if link and not link.startswith(\"http\"):\n",
    "        link = url + link  # Convertir enlaces relativos en absolutos\n",
    "    datasets.append({\"Título\": title, \"Enlace\": link})\n",
    "\n",
    "# Extraer los enlaces a los archivos CSS\n",
    "css_links = []\n",
    "for link in soup.find_all(\"link\", rel=\"stylesheet\"):  # Buscamos solo los enlaces de tipo CSS\n",
    "    href = link.get(\"href\")\n",
    "    if href:\n",
    "        if not href.startswith(\"http\"):\n",
    "            href = url + href  # Asumiendo que el enlace es relativo y necesitamos convertirlo\n",
    "        css_links.append({\"Archivo CSS\": href})\n",
    "\n",
    "# Combinar los datasets y los enlaces de los archivos CSS en un solo DataFrame\n",
    "all_data = datasets + css_links\n",
    "\n",
    "# Guardar todo en un archivo CSV\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(\"datasets_datosabiertos_bs.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"Datos guardados en 'datasets_datosabiertos_bs.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selenium\n",
    "\n",
    "**Procedimiento:**\n",
    "\n",
    "1. Configurar Selenium con el controlador del navegador Chrome.\n",
    "\n",
    "2. Cargar la página principal del sitio web.\n",
    "\n",
    "3. Identificar los elementos que contienen los títulos y enlaces mediante selectores CSS.\n",
    "\n",
    "4. Extraer los datos y guardarlos en un archivo CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements found: 1\n",
      "Title: , Link: https://datosabiertos.gob.ec/webassets/vendor/0b01aef1_font-awesome.css\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Abrir el sitio web\n",
    "url = \"https://datosabiertos.gob.ec/\"\n",
    "driver.get(url)\n",
    "time.sleep(10)  # Esperar a que cargue el contenido\n",
    "\n",
    "# Extraer títulos y enlaces\n",
    "datasets = []\n",
    "elements = driver.find_elements(By.CSS_SELECTOR, \"body > link:nth-child(8)\")\n",
    "\n",
    "# Debug: Print the number of elements found\n",
    "print(f\"Number of elements found: {len(elements)}\")\n",
    "\n",
    "for element in elements:\n",
    "    title = element.text.strip()\n",
    "    link = element.get_attribute(\"href\")\n",
    "    datasets.append({\"Título\": title, \"Enlace\": link})\n",
    "\n",
    "    # Debug: Print each title and link\n",
    "    print(f\"Title: {title}, Link: {link}\")\n",
    "\n",
    "# Guardar en un archivo CSV\n",
    "df = pd.DataFrame(datasets)\n",
    "df.to_csv(\"datasets_datosabiertos_selenium.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **BeautifulSoup:** Se logró extraer los títulos y enlaces de los datasets disponibles en la página. Los datos se guardaron en el archivo  datasets_datosabiertos_bs.csv.\n",
    "\n",
    "- **Selenium:** La interacción con la página permitió obtener los mismos datos y guardarlos en el archivo datasets_datosabiertos_selenium.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "- BeautifulSoup es ideal para sitios con contenido estático, ofreciendo simplicidad y rapidez.\n",
    "\n",
    "- Selenium es necesario para interactuar con páginas que dependen de JavaScript, aunque es más lenta y compleja.\n",
    "\n",
    "- Ambos métodos lograron extraer los datos requeridos y guardarlos exitosamente en archivos CSV.\n",
    "\n",
    "- Se recomienda utilizar BeautifulSoup cuando sea posible, priorizando Selenium solo para casos específicos.\n",
    "\n",
    "## Recomendaciones\n",
    "\n",
    "- Verificar los términos de uso del sitio web antes de realizar scraping.\n",
    "\n",
    "- Automatizar la ejecución periódica del código si se requiere mantener los datos actualizados.\n",
    "\n",
    "- Optimizar el uso de Selenium para reducir el tiempo de espera y mejorar el rendimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GitHub:** git@github.com: alda244\n",
    "\n",
    "\n",
    "\n",
    "[GitHub Métodos Númericos - Repositorio](https://github.com/alda244/MN_ActividadesExtra/blob/main/MN_AE12/PereiraAlicia_ActividadExtra12.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
